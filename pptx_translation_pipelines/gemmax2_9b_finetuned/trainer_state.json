{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.999,
  "eval_steps": 500,
  "global_step": 999,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.026785714285714284,
      "grad_norm": 0.34417086839675903,
      "learning_rate": 3.5714285714285714e-06,
      "loss": 0.4011,
      "step": 5
    },
    {
      "epoch": 0.05357142857142857,
      "grad_norm": 0.40307748317718506,
      "learning_rate": 8.035714285714286e-06,
      "loss": 0.3618,
      "step": 10
    },
    {
      "epoch": 0.08035714285714286,
      "grad_norm": 1.2871580123901367,
      "learning_rate": 1.25e-05,
      "loss": 0.3322,
      "step": 15
    },
    {
      "epoch": 0.10714285714285714,
      "grad_norm": 0.3947218656539917,
      "learning_rate": 1.6964285714285715e-05,
      "loss": 0.329,
      "step": 20
    },
    {
      "epoch": 0.13392857142857142,
      "grad_norm": 0.3339184820652008,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 0.3331,
      "step": 25
    },
    {
      "epoch": 0.16071428571428573,
      "grad_norm": 0.2863442301750183,
      "learning_rate": 2.5892857142857148e-05,
      "loss": 0.3266,
      "step": 30
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.36534130573272705,
      "learning_rate": 3.0357142857142857e-05,
      "loss": 0.3163,
      "step": 35
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 1.197726845741272,
      "learning_rate": 3.4821428571428574e-05,
      "loss": 0.3322,
      "step": 40
    },
    {
      "epoch": 0.24107142857142858,
      "grad_norm": 0.47362589836120605,
      "learning_rate": 3.928571428571429e-05,
      "loss": 0.3451,
      "step": 45
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 0.35112741589546204,
      "learning_rate": 4.375e-05,
      "loss": 0.3615,
      "step": 50
    },
    {
      "epoch": 0.29464285714285715,
      "grad_norm": 0.41644343733787537,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 0.3623,
      "step": 55
    },
    {
      "epoch": 0.32142857142857145,
      "grad_norm": 0.3043197691440582,
      "learning_rate": 4.999559412596081e-05,
      "loss": 0.3168,
      "step": 60
    },
    {
      "epoch": 0.3482142857142857,
      "grad_norm": 0.513023316860199,
      "learning_rate": 4.9968674963768884e-05,
      "loss": 0.3112,
      "step": 65
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.29561033844947815,
      "learning_rate": 4.9917310669950814e-05,
      "loss": 0.3001,
      "step": 70
    },
    {
      "epoch": 0.4017857142857143,
      "grad_norm": 0.38687923550605774,
      "learning_rate": 4.9841551531793576e-05,
      "loss": 0.2846,
      "step": 75
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.4580487608909607,
      "learning_rate": 4.974147171991723e-05,
      "loss": 0.3235,
      "step": 80
    },
    {
      "epoch": 0.45535714285714285,
      "grad_norm": 0.43643924593925476,
      "learning_rate": 4.9617169215659506e-05,
      "loss": 0.3687,
      "step": 85
    },
    {
      "epoch": 0.48214285714285715,
      "grad_norm": 0.5060898065567017,
      "learning_rate": 4.9468765715148956e-05,
      "loss": 0.326,
      "step": 90
    },
    {
      "epoch": 0.5089285714285714,
      "grad_norm": 0.7105429172515869,
      "learning_rate": 4.929640651016053e-05,
      "loss": 0.3212,
      "step": 95
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 0.4753967225551605,
      "learning_rate": 4.910026034587031e-05,
      "loss": 0.3338,
      "step": 100
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.5699114799499512,
      "learning_rate": 4.888051925564853e-05,
      "loss": 0.298,
      "step": 105
    },
    {
      "epoch": 0.5892857142857143,
      "grad_norm": 0.3803268373012543,
      "learning_rate": 4.863739837305282e-05,
      "loss": 0.3535,
      "step": 110
    },
    {
      "epoch": 0.6160714285714286,
      "grad_norm": 0.6151545643806458,
      "learning_rate": 4.837113572120555e-05,
      "loss": 0.2698,
      "step": 115
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 0.725273072719574,
      "learning_rate": 4.808199197976157e-05,
      "loss": 0.3247,
      "step": 120
    },
    {
      "epoch": 0.6696428571428571,
      "grad_norm": 0.5227991342544556,
      "learning_rate": 4.777025022969447e-05,
      "loss": 0.2745,
      "step": 125
    },
    {
      "epoch": 0.6964285714285714,
      "grad_norm": 0.6045516133308411,
      "learning_rate": 4.743621567615129e-05,
      "loss": 0.3445,
      "step": 130
    },
    {
      "epoch": 0.7232142857142857,
      "grad_norm": 0.3215738534927368,
      "learning_rate": 4.708021534964694e-05,
      "loss": 0.311,
      "step": 135
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.5047926306724548,
      "learning_rate": 4.670259778589074e-05,
      "loss": 0.3492,
      "step": 140
    },
    {
      "epoch": 0.7767857142857143,
      "grad_norm": 0.3665695786476135,
      "learning_rate": 4.630373268455895e-05,
      "loss": 0.3192,
      "step": 145
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 0.5771991014480591,
      "learning_rate": 4.58840105473469e-05,
      "loss": 0.3252,
      "step": 150
    },
    {
      "epoch": 0.8303571428571429,
      "grad_norm": 0.3623183071613312,
      "learning_rate": 4.54438422956553e-05,
      "loss": 0.3443,
      "step": 155
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.36657723784446716,
      "learning_rate": 4.498365886828511e-05,
      "loss": 0.2684,
      "step": 160
    },
    {
      "epoch": 0.8839285714285714,
      "grad_norm": 0.4925421476364136,
      "learning_rate": 4.450391079953463e-05,
      "loss": 0.2879,
      "step": 165
    },
    {
      "epoch": 0.9107142857142857,
      "grad_norm": 0.6105939149856567,
      "learning_rate": 4.400506777811201e-05,
      "loss": 0.2787,
      "step": 170
    },
    {
      "epoch": 0.9375,
      "grad_norm": 0.39603689312934875,
      "learning_rate": 4.3487618187294986e-05,
      "loss": 0.2846,
      "step": 175
    },
    {
      "epoch": 0.9642857142857143,
      "grad_norm": 0.9165802597999573,
      "learning_rate": 4.295206862678802e-05,
      "loss": 0.3419,
      "step": 180
    },
    {
      "epoch": 0.9910714285714286,
      "grad_norm": 1.1341731548309326,
      "learning_rate": 4.2398943416744976e-05,
      "loss": 0.2476,
      "step": 185
    },
    {
      "epoch": 1.0160714285714285,
      "grad_norm": 0.4534240961074829,
      "learning_rate": 4.182878408444293e-05,
      "loss": 0.2594,
      "step": 190
    },
    {
      "epoch": 1.042857142857143,
      "grad_norm": 0.580409824848175,
      "learning_rate": 4.124214883410963e-05,
      "loss": 0.1313,
      "step": 195
    },
    {
      "epoch": 1.0696428571428571,
      "grad_norm": 0.39687788486480713,
      "learning_rate": 4.063961200042368e-05,
      "loss": 0.1417,
      "step": 200
    },
    {
      "epoch": 1.0964285714285715,
      "grad_norm": 0.46109268069267273,
      "learning_rate": 4.002176348622258e-05,
      "loss": 0.2001,
      "step": 205
    },
    {
      "epoch": 1.1232142857142857,
      "grad_norm": 0.46513232588768005,
      "learning_rate": 3.938920818496888e-05,
      "loss": 0.1787,
      "step": 210
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.6992481350898743,
      "learning_rate": 3.874256538854022e-05,
      "loss": 0.1846,
      "step": 215
    },
    {
      "epoch": 1.1767857142857143,
      "grad_norm": 0.6878955364227295,
      "learning_rate": 3.8082468180922686e-05,
      "loss": 0.1298,
      "step": 220
    },
    {
      "epoch": 1.2035714285714285,
      "grad_norm": 0.36650073528289795,
      "learning_rate": 3.74095628184014e-05,
      "loss": 0.1339,
      "step": 225
    },
    {
      "epoch": 1.230357142857143,
      "grad_norm": 0.33847519755363464,
      "learning_rate": 3.672450809685489e-05,
      "loss": 0.1568,
      "step": 230
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 0.6380257606506348,
      "learning_rate": 3.60279747067729e-05,
      "loss": 0.1931,
      "step": 235
    },
    {
      "epoch": 1.2839285714285715,
      "grad_norm": 0.4178067743778229,
      "learning_rate": 3.532064457662889e-05,
      "loss": 0.1505,
      "step": 240
    },
    {
      "epoch": 1.3107142857142857,
      "grad_norm": 0.5466834902763367,
      "learning_rate": 3.4603210205250274e-05,
      "loss": 0.171,
      "step": 245
    },
    {
      "epoch": 1.3375,
      "grad_norm": 0.48132264614105225,
      "learning_rate": 3.387637398383991e-05,
      "loss": 0.1629,
      "step": 250
    },
    {
      "epoch": 1.3642857142857143,
      "grad_norm": 0.5662115216255188,
      "learning_rate": 3.314084750831263e-05,
      "loss": 0.1246,
      "step": 255
    },
    {
      "epoch": 1.3910714285714285,
      "grad_norm": 0.3606679439544678,
      "learning_rate": 3.239735088262007e-05,
      "loss": 0.1173,
      "step": 260
    },
    {
      "epoch": 1.417857142857143,
      "grad_norm": 0.6317269802093506,
      "learning_rate": 3.1646612013745904e-05,
      "loss": 0.151,
      "step": 265
    },
    {
      "epoch": 1.4446428571428571,
      "grad_norm": 0.5040529370307922,
      "learning_rate": 3.088936589906155e-05,
      "loss": 0.1679,
      "step": 270
    },
    {
      "epoch": 1.4714285714285715,
      "grad_norm": 0.6305214166641235,
      "learning_rate": 3.0126353906740274e-05,
      "loss": 0.1803,
      "step": 275
    },
    {
      "epoch": 1.4982142857142857,
      "grad_norm": 0.6481413841247559,
      "learning_rate": 2.935832304993402e-05,
      "loss": 0.1513,
      "step": 280
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.6707467436790466,
      "learning_rate": 2.8586025255423655e-05,
      "loss": 0.1627,
      "step": 285
    },
    {
      "epoch": 1.5517857142857143,
      "grad_norm": 1.0863763093948364,
      "learning_rate": 2.7810216627458535e-05,
      "loss": 0.1663,
      "step": 290
    },
    {
      "epoch": 1.5785714285714287,
      "grad_norm": 0.7222505211830139,
      "learning_rate": 2.703165670750632e-05,
      "loss": 0.2091,
      "step": 295
    },
    {
      "epoch": 1.6053571428571427,
      "grad_norm": 0.5739521384239197,
      "learning_rate": 2.625110773063754e-05,
      "loss": 0.1629,
      "step": 300
    },
    {
      "epoch": 1.6321428571428571,
      "grad_norm": 0.6454166173934937,
      "learning_rate": 2.546933387927309e-05,
      "loss": 0.1746,
      "step": 305
    },
    {
      "epoch": 1.6589285714285715,
      "grad_norm": 0.47963327169418335,
      "learning_rate": 2.4687100535025172e-05,
      "loss": 0.1332,
      "step": 310
    },
    {
      "epoch": 1.6857142857142857,
      "grad_norm": 0.41893118619918823,
      "learning_rate": 2.3905173529364227e-05,
      "loss": 0.1529,
      "step": 315
    },
    {
      "epoch": 1.7125,
      "grad_norm": 0.6105039715766907,
      "learning_rate": 2.312431839384543e-05,
      "loss": 0.1646,
      "step": 320
    },
    {
      "epoch": 1.7392857142857143,
      "grad_norm": 0.5076176524162292,
      "learning_rate": 2.2345299610628767e-05,
      "loss": 0.1284,
      "step": 325
    },
    {
      "epoch": 1.7660714285714287,
      "grad_norm": 0.8720279335975647,
      "learning_rate": 2.1568879864026602e-05,
      "loss": 0.1337,
      "step": 330
    },
    {
      "epoch": 1.7928571428571427,
      "grad_norm": 0.49323710799217224,
      "learning_rate": 2.079581929381127e-05,
      "loss": 0.134,
      "step": 335
    },
    {
      "epoch": 1.8196428571428571,
      "grad_norm": 0.6033639311790466,
      "learning_rate": 2.0026874751013972e-05,
      "loss": 0.2032,
      "step": 340
    },
    {
      "epoch": 1.8464285714285715,
      "grad_norm": 0.5177760720252991,
      "learning_rate": 1.9262799056943377e-05,
      "loss": 0.1594,
      "step": 345
    },
    {
      "epoch": 1.8732142857142857,
      "grad_norm": 0.44473230838775635,
      "learning_rate": 1.850434026614948e-05,
      "loss": 0.1469,
      "step": 350
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.549008309841156,
      "learning_rate": 1.7752240934054248e-05,
      "loss": 0.1795,
      "step": 355
    },
    {
      "epoch": 1.9267857142857143,
      "grad_norm": 0.6057127118110657,
      "learning_rate": 1.7007237389966146e-05,
      "loss": 0.1474,
      "step": 360
    },
    {
      "epoch": 1.9535714285714287,
      "grad_norm": 0.5804510712623596,
      "learning_rate": 1.627005901619013e-05,
      "loss": 0.175,
      "step": 365
    },
    {
      "epoch": 1.9803571428571427,
      "grad_norm": 0.5438113808631897,
      "learning_rate": 1.554142753393909e-05,
      "loss": 0.1581,
      "step": 370
    },
    {
      "epoch": 2.005357142857143,
      "grad_norm": 0.45283132791519165,
      "learning_rate": 1.4822056296745656e-05,
      "loss": 0.1161,
      "step": 375
    },
    {
      "epoch": 2.032142857142857,
      "grad_norm": 0.37624692916870117,
      "learning_rate": 1.4112649592066293e-05,
      "loss": 0.0472,
      "step": 380
    },
    {
      "epoch": 2.0589285714285714,
      "grad_norm": 0.7807841897010803,
      "learning_rate": 1.341390195176131e-05,
      "loss": 0.0785,
      "step": 385
    },
    {
      "epoch": 2.085714285714286,
      "grad_norm": 0.9218952059745789,
      "learning_rate": 1.2726497472125981e-05,
      "loss": 0.046,
      "step": 390
    },
    {
      "epoch": 2.1125,
      "grad_norm": 0.5492798089981079,
      "learning_rate": 1.20511091441384e-05,
      "loss": 0.0518,
      "step": 395
    },
    {
      "epoch": 2.1392857142857142,
      "grad_norm": 0.4815712571144104,
      "learning_rate": 1.1388398194579725e-05,
      "loss": 0.042,
      "step": 400
    },
    {
      "epoch": 2.1660714285714286,
      "grad_norm": 0.5539759993553162,
      "learning_rate": 1.0739013438672043e-05,
      "loss": 0.054,
      "step": 405
    },
    {
      "epoch": 2.192857142857143,
      "grad_norm": 0.8162668943405151,
      "learning_rate": 1.0103590644867537e-05,
      "loss": 0.0507,
      "step": 410
    },
    {
      "epoch": 2.219642857142857,
      "grad_norm": 0.647520899772644,
      "learning_rate": 9.482751912410748e-06,
      "loss": 0.0517,
      "step": 415
    },
    {
      "epoch": 2.2464285714285714,
      "grad_norm": 0.6551623344421387,
      "learning_rate": 8.877105062283609e-06,
      "loss": 0.0321,
      "step": 420
    },
    {
      "epoch": 2.273214285714286,
      "grad_norm": 0.9164025783538818,
      "learning_rate": 8.287243042129183e-06,
      "loss": 0.0501,
      "step": 425
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.6111375093460083,
      "learning_rate": 7.71374334573697e-06,
      "loss": 0.0521,
      "step": 430
    },
    {
      "epoch": 2.3267857142857142,
      "grad_norm": 0.39352548122406006,
      "learning_rate": 7.157167447658047e-06,
      "loss": 0.0322,
      "step": 435
    },
    {
      "epoch": 2.3535714285714286,
      "grad_norm": 0.6146036982536316,
      "learning_rate": 6.618060253503472e-06,
      "loss": 0.048,
      "step": 440
    },
    {
      "epoch": 2.380357142857143,
      "grad_norm": 0.3121899366378784,
      "learning_rate": 6.0969495664642685e-06,
      "loss": 0.0488,
      "step": 445
    },
    {
      "epoch": 2.407142857142857,
      "grad_norm": 0.39120393991470337,
      "learning_rate": 5.594345570575227e-06,
      "loss": 0.0608,
      "step": 450
    },
    {
      "epoch": 2.4339285714285714,
      "grad_norm": 0.34929177165031433,
      "learning_rate": 5.110740331228423e-06,
      "loss": 0.0415,
      "step": 455
    },
    {
      "epoch": 2.460714285714286,
      "grad_norm": 0.47201845049858093,
      "learning_rate": 4.646607313425419e-06,
      "loss": 0.0336,
      "step": 460
    },
    {
      "epoch": 2.4875,
      "grad_norm": 0.8018332123756409,
      "learning_rate": 4.202400918239904e-06,
      "loss": 0.0635,
      "step": 465
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 0.8823233246803284,
      "learning_rate": 3.778556037944517e-06,
      "loss": 0.0589,
      "step": 470
    },
    {
      "epoch": 2.5410714285714286,
      "grad_norm": 0.780794620513916,
      "learning_rate": 3.375487630237359e-06,
      "loss": 0.051,
      "step": 475
    },
    {
      "epoch": 2.567857142857143,
      "grad_norm": 0.5640268921852112,
      "learning_rate": 2.9935903119851757e-06,
      "loss": 0.0488,
      "step": 480
    },
    {
      "epoch": 2.594642857142857,
      "grad_norm": 0.5313294529914856,
      "learning_rate": 2.633237972880781e-06,
      "loss": 0.0443,
      "step": 485
    },
    {
      "epoch": 2.6214285714285714,
      "grad_norm": 0.5428314208984375,
      "learning_rate": 2.2947834093931142e-06,
      "loss": 0.0499,
      "step": 490
    },
    {
      "epoch": 2.648214285714286,
      "grad_norm": 1.5839455127716064,
      "learning_rate": 1.9785579793681867e-06,
      "loss": 0.0688,
      "step": 495
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.6043213605880737,
      "learning_rate": 1.6848712776191767e-06,
      "loss": 0.0402,
      "step": 500
    },
    {
      "epoch": 2.7017857142857142,
      "grad_norm": 0.48149892687797546,
      "learning_rate": 1.4140108328231704e-06,
      "loss": 0.049,
      "step": 505
    },
    {
      "epoch": 2.7285714285714286,
      "grad_norm": 0.5808730125427246,
      "learning_rate": 1.1662418260214137e-06,
      "loss": 0.0316,
      "step": 510
    },
    {
      "epoch": 2.755357142857143,
      "grad_norm": 0.7377156019210815,
      "learning_rate": 9.418068309985733e-07,
      "loss": 0.0574,
      "step": 515
    },
    {
      "epoch": 2.782142857142857,
      "grad_norm": 0.44662612676620483,
      "learning_rate": 7.409255767952538e-07,
      "loss": 0.0452,
      "step": 520
    },
    {
      "epoch": 2.8089285714285714,
      "grad_norm": 0.4789251685142517,
      "learning_rate": 5.637947325862258e-07,
      "loss": 0.0471,
      "step": 525
    },
    {
      "epoch": 2.835714285714286,
      "grad_norm": 0.49537214636802673,
      "learning_rate": 4.1058771513500794e-07,
      "loss": 0.0737,
      "step": 530
    },
    {
      "epoch": 2.8625,
      "grad_norm": 0.35006603598594666,
      "learning_rate": 2.814545190133272e-07,
      "loss": 0.0618,
      "step": 535
    },
    {
      "epoch": 2.8892857142857142,
      "grad_norm": 0.37139326333999634,
      "learning_rate": 1.7652156975161982e-07,
      "loss": 0.0387,
      "step": 540
    },
    {
      "epoch": 2.9160714285714286,
      "grad_norm": 0.5263131856918335,
      "learning_rate": 9.589160006438225e-08,
      "loss": 0.0369,
      "step": 545
    },
    {
      "epoch": 2.942857142857143,
      "grad_norm": 0.4125349819660187,
      "learning_rate": 3.9643549271584094e-08,
      "loss": 0.0445,
      "step": 550
    },
    {
      "epoch": 2.969642857142857,
      "grad_norm": 0.44638827443122864,
      "learning_rate": 7.832486014522355e-09,
      "loss": 0.0568,
      "step": 555
    },
    {
      "epoch": 1.7989304812834224,
      "grad_norm": 3.067371368408203,
      "learning_rate": 2.0761265122525052e-05,
      "loss": 0.2871,
      "step": 560
    },
    {
      "epoch": 1.8149732620320855,
      "grad_norm": 0.5869042873382568,
      "learning_rate": 2.030075547917241e-05,
      "loss": 0.1548,
      "step": 565
    },
    {
      "epoch": 1.8310160427807487,
      "grad_norm": 0.9668304324150085,
      "learning_rate": 1.9841892978384778e-05,
      "loss": 0.1477,
      "step": 570
    },
    {
      "epoch": 1.8470588235294119,
      "grad_norm": 0.6261864304542542,
      "learning_rate": 1.9384838457069714e-05,
      "loss": 0.172,
      "step": 575
    },
    {
      "epoch": 1.863101604278075,
      "grad_norm": 0.42421892285346985,
      "learning_rate": 1.8929752118415882e-05,
      "loss": 0.0989,
      "step": 580
    },
    {
      "epoch": 1.879144385026738,
      "grad_norm": 0.5013754367828369,
      "learning_rate": 1.8476793475739917e-05,
      "loss": 0.0901,
      "step": 585
    },
    {
      "epoch": 1.895187165775401,
      "grad_norm": 1.2022736072540283,
      "learning_rate": 1.8026121296575025e-05,
      "loss": 0.1841,
      "step": 590
    },
    {
      "epoch": 1.911229946524064,
      "grad_norm": 0.5239272713661194,
      "learning_rate": 1.7577893547021058e-05,
      "loss": 0.2339,
      "step": 595
    },
    {
      "epoch": 1.9272727272727272,
      "grad_norm": 1.0646929740905762,
      "learning_rate": 1.713226733637544e-05,
      "loss": 0.2204,
      "step": 600
    },
    {
      "epoch": 1.9433155080213904,
      "grad_norm": 0.44676971435546875,
      "learning_rate": 1.668939886206446e-05,
      "loss": 0.2269,
      "step": 605
    },
    {
      "epoch": 1.9593582887700536,
      "grad_norm": 0.46933692693710327,
      "learning_rate": 1.624944335489415e-05,
      "loss": 0.2857,
      "step": 610
    },
    {
      "epoch": 1.9754010695187165,
      "grad_norm": 0.5893728137016296,
      "learning_rate": 1.5812555024640014e-05,
      "loss": 0.1112,
      "step": 615
    },
    {
      "epoch": 1.9914438502673797,
      "grad_norm": 0.6911211013793945,
      "learning_rate": 1.537888700599461e-05,
      "loss": 0.1066,
      "step": 620
    },
    {
      "epoch": 2.009625668449198,
      "grad_norm": 0.42422306537628174,
      "learning_rate": 1.4948591304891952e-05,
      "loss": 0.2382,
      "step": 625
    },
    {
      "epoch": 2.025668449197861,
      "grad_norm": 0.26685404777526855,
      "learning_rate": 1.45218187452276e-05,
      "loss": 0.0449,
      "step": 630
    },
    {
      "epoch": 2.041711229946524,
      "grad_norm": 1.6921093463897705,
      "learning_rate": 1.4098718915993042e-05,
      "loss": 0.1742,
      "step": 635
    },
    {
      "epoch": 2.057754010695187,
      "grad_norm": 0.627653181552887,
      "learning_rate": 1.3679440118842921e-05,
      "loss": 0.212,
      "step": 640
    },
    {
      "epoch": 2.07379679144385,
      "grad_norm": 2.2205841541290283,
      "learning_rate": 1.3264129316113489e-05,
      "loss": 0.1882,
      "step": 645
    },
    {
      "epoch": 2.0898395721925134,
      "grad_norm": 0.44789260625839233,
      "learning_rate": 1.2852932079310503e-05,
      "loss": 0.1161,
      "step": 650
    },
    {
      "epoch": 2.1058823529411765,
      "grad_norm": 0.41405731439590454,
      "learning_rate": 1.2445992538084677e-05,
      "loss": 0.1285,
      "step": 655
    },
    {
      "epoch": 2.1219251336898397,
      "grad_norm": 1.5251514911651611,
      "learning_rate": 1.204345332971245e-05,
      "loss": 0.1781,
      "step": 660
    },
    {
      "epoch": 2.137967914438503,
      "grad_norm": 0.46181347966194153,
      "learning_rate": 1.164545554909986e-05,
      "loss": 0.1423,
      "step": 665
    },
    {
      "epoch": 2.1540106951871656,
      "grad_norm": 0.8239405751228333,
      "learning_rate": 1.1252138699327106e-05,
      "loss": 0.2268,
      "step": 670
    },
    {
      "epoch": 2.1700534759358288,
      "grad_norm": 0.858978807926178,
      "learning_rate": 1.0863640642750994e-05,
      "loss": 0.1021,
      "step": 675
    },
    {
      "epoch": 2.186096256684492,
      "grad_norm": 0.5736316442489624,
      "learning_rate": 1.0480097552682466e-05,
      "loss": 0.2163,
      "step": 680
    },
    {
      "epoch": 2.202139037433155,
      "grad_norm": 0.8466852903366089,
      "learning_rate": 1.010164386565626e-05,
      "loss": 0.0981,
      "step": 685
    },
    {
      "epoch": 2.2181818181818183,
      "grad_norm": 1.1273409128189087,
      "learning_rate": 9.72841223430918e-06,
      "loss": 0.2361,
      "step": 690
    },
    {
      "epoch": 2.2342245989304814,
      "grad_norm": 0.6816837787628174,
      "learning_rate": 9.360533480883813e-06,
      "loss": 0.1015,
      "step": 695
    },
    {
      "epoch": 2.250267379679144,
      "grad_norm": 0.4538503885269165,
      "learning_rate": 8.998136551373681e-06,
      "loss": 0.085,
      "step": 700
    },
    {
      "epoch": 2.2663101604278073,
      "grad_norm": 0.8893122673034668,
      "learning_rate": 8.641348470326094e-06,
      "loss": 0.1475,
      "step": 705
    },
    {
      "epoch": 2.2823529411764705,
      "grad_norm": 0.9312331080436707,
      "learning_rate": 8.29029429631854e-06,
      "loss": 0.169,
      "step": 710
    },
    {
      "epoch": 2.2983957219251336,
      "grad_norm": 0.7333142161369324,
      "learning_rate": 7.94509707812409e-06,
      "loss": 0.1032,
      "step": 715
    },
    {
      "epoch": 2.314438502673797,
      "grad_norm": 0.2979028820991516,
      "learning_rate": 7.605877811581294e-06,
      "loss": 0.1524,
      "step": 720
    },
    {
      "epoch": 2.33048128342246,
      "grad_norm": 1.1444464921951294,
      "learning_rate": 7.272755397183706e-06,
      "loss": 0.2102,
      "step": 725
    },
    {
      "epoch": 2.346524064171123,
      "grad_norm": 0.9341723918914795,
      "learning_rate": 6.945846598403761e-06,
      "loss": 0.1614,
      "step": 730
    },
    {
      "epoch": 2.362566844919786,
      "grad_norm": 0.6538821458816528,
      "learning_rate": 6.625266000765784e-06,
      "loss": 0.108,
      "step": 735
    },
    {
      "epoch": 2.378609625668449,
      "grad_norm": 0.6604092717170715,
      "learning_rate": 6.311125971682366e-06,
      "loss": 0.1514,
      "step": 740
    },
    {
      "epoch": 2.394652406417112,
      "grad_norm": 0.44333571195602417,
      "learning_rate": 6.0035366210681785e-06,
      "loss": 0.1036,
      "step": 745
    },
    {
      "epoch": 2.4106951871657754,
      "grad_norm": 0.5447622537612915,
      "learning_rate": 5.702605762745153e-06,
      "loss": 0.1794,
      "step": 750
    },
    {
      "epoch": 2.4267379679144385,
      "grad_norm": 0.4970736503601074,
      "learning_rate": 5.408438876652389e-06,
      "loss": 0.1083,
      "step": 755
    },
    {
      "epoch": 2.4427807486631017,
      "grad_norm": 1.3080379962921143,
      "learning_rate": 5.121139071874154e-06,
      "loss": 0.1834,
      "step": 760
    },
    {
      "epoch": 2.458823529411765,
      "grad_norm": 0.8151113390922546,
      "learning_rate": 4.840807050498903e-06,
      "loss": 0.2247,
      "step": 765
    },
    {
      "epoch": 2.4748663101604276,
      "grad_norm": 0.7096529603004456,
      "learning_rate": 4.567541072321935e-06,
      "loss": 0.157,
      "step": 770
    },
    {
      "epoch": 2.4909090909090907,
      "grad_norm": 0.35564783215522766,
      "learning_rate": 4.301436920404189e-06,
      "loss": 0.1386,
      "step": 775
    },
    {
      "epoch": 2.506951871657754,
      "grad_norm": 0.568743884563446,
      "learning_rate": 4.042587867499045e-06,
      "loss": 0.0752,
      "step": 780
    },
    {
      "epoch": 2.522994652406417,
      "grad_norm": 0.9073476791381836,
      "learning_rate": 3.7910846433591524e-06,
      "loss": 0.2269,
      "step": 785
    },
    {
      "epoch": 2.5390374331550802,
      "grad_norm": 0.7337488532066345,
      "learning_rate": 3.5470154029345004e-06,
      "loss": 0.0785,
      "step": 790
    },
    {
      "epoch": 2.5550802139037434,
      "grad_norm": 0.575691819190979,
      "learning_rate": 3.310465695473039e-06,
      "loss": 0.1224,
      "step": 795
    },
    {
      "epoch": 2.5711229946524066,
      "grad_norm": 0.9745825529098511,
      "learning_rate": 3.081518434534633e-06,
      "loss": 0.159,
      "step": 800
    },
    {
      "epoch": 2.5871657754010693,
      "grad_norm": 0.5938615202903748,
      "learning_rate": 2.860253868928853e-06,
      "loss": 0.0556,
      "step": 805
    },
    {
      "epoch": 2.6032085561497325,
      "grad_norm": 0.5486441850662231,
      "learning_rate": 2.6467495545867583e-06,
      "loss": 0.1109,
      "step": 810
    },
    {
      "epoch": 2.6192513368983956,
      "grad_norm": 0.9077640771865845,
      "learning_rate": 2.4410803273766354e-06,
      "loss": 0.1316,
      "step": 815
    },
    {
      "epoch": 2.635294117647059,
      "grad_norm": 0.5628881454467773,
      "learning_rate": 2.243318276873077e-06,
      "loss": 0.1695,
      "step": 820
    },
    {
      "epoch": 2.651336898395722,
      "grad_norm": 1.079730749130249,
      "learning_rate": 2.0535327210887385e-06,
      "loss": 0.248,
      "step": 825
    },
    {
      "epoch": 2.667379679144385,
      "grad_norm": 0.5329406261444092,
      "learning_rate": 1.8717901821775019e-06,
      "loss": 0.2276,
      "step": 830
    },
    {
      "epoch": 2.6834224598930483,
      "grad_norm": 2.7885947227478027,
      "learning_rate": 1.6981543631176627e-06,
      "loss": 0.1856,
      "step": 835
    },
    {
      "epoch": 2.699465240641711,
      "grad_norm": 0.9654947519302368,
      "learning_rate": 1.5326861253832508e-06,
      "loss": 0.1653,
      "step": 840
    },
    {
      "epoch": 2.715508021390374,
      "grad_norm": 0.6121118664741516,
      "learning_rate": 1.375443467611362e-06,
      "loss": 0.2288,
      "step": 845
    },
    {
      "epoch": 2.7315508021390373,
      "grad_norm": 0.6635532975196838,
      "learning_rate": 1.2264815052728835e-06,
      "loss": 0.1304,
      "step": 850
    },
    {
      "epoch": 2.7475935828877005,
      "grad_norm": 0.5672215819358826,
      "learning_rate": 1.0858524513539052e-06,
      "loss": 0.082,
      "step": 855
    },
    {
      "epoch": 2.7636363636363637,
      "grad_norm": 0.7922463417053223,
      "learning_rate": 9.536055980543773e-07,
      "loss": 0.139,
      "step": 860
    },
    {
      "epoch": 2.779679144385027,
      "grad_norm": 0.5935091376304626,
      "learning_rate": 8.297872995106343e-07,
      "loss": 0.1853,
      "step": 865
    },
    {
      "epoch": 2.79572192513369,
      "grad_norm": 0.862916886806488,
      "learning_rate": 7.144409555476849e-07,
      "loss": 0.0627,
      "step": 870
    },
    {
      "epoch": 2.8117647058823527,
      "grad_norm": 0.6803439259529114,
      "learning_rate": 6.076069964670305e-07,
      "loss": 0.2777,
      "step": 875
    },
    {
      "epoch": 2.827807486631016,
      "grad_norm": 1.3829742670059204,
      "learning_rate": 5.093228688753893e-07,
      "loss": 0.2045,
      "step": 880
    },
    {
      "epoch": 2.843850267379679,
      "grad_norm": 0.6756134629249573,
      "learning_rate": 4.196230225591652e-07,
      "loss": 0.1092,
      "step": 885
    },
    {
      "epoch": 2.859893048128342,
      "grad_norm": 0.7609472870826721,
      "learning_rate": 3.3853889840939346e-07,
      "loss": 0.1716,
      "step": 890
    },
    {
      "epoch": 2.8759358288770054,
      "grad_norm": 0.5863254070281982,
      "learning_rate": 2.660989174013334e-07,
      "loss": 0.1462,
      "step": 895
    },
    {
      "epoch": 2.8919786096256686,
      "grad_norm": 0.48584672808647156,
      "learning_rate": 2.0232847063255123e-07,
      "loss": 0.1817,
      "step": 900
    },
    {
      "epoch": 2.9080213903743317,
      "grad_norm": 0.7783153057098389,
      "learning_rate": 1.4724991042303903e-07,
      "loss": 0.0973,
      "step": 905
    },
    {
      "epoch": 2.9240641711229944,
      "grad_norm": 0.8777063488960266,
      "learning_rate": 1.0088254248045715e-07,
      "loss": 0.1237,
      "step": 910
    },
    {
      "epoch": 2.9401069518716576,
      "grad_norm": 0.6943666338920593,
      "learning_rate": 6.324261913324691e-08,
      "loss": 0.1486,
      "step": 915
    },
    {
      "epoch": 2.9561497326203208,
      "grad_norm": 1.4916009902954102,
      "learning_rate": 3.43433336339849e-08,
      "loss": 0.1841,
      "step": 920
    },
    {
      "epoch": 2.972192513368984,
      "grad_norm": 0.5294527411460876,
      "learning_rate": 1.4194815534993022e-08,
      "loss": 0.1189,
      "step": 925
    },
    {
      "epoch": 2.988235294117647,
      "grad_norm": 0.8043996095657349,
      "learning_rate": 2.804127137803758e-09,
      "loss": 0.1449,
      "step": 930
    },
    {
      "epoch": 2.807,
      "grad_norm": 0.42060574889183044,
      "learning_rate": 6.421685047525938e-07,
      "loss": 0.0225,
      "step": 935
    },
    {
      "epoch": 2.822,
      "grad_norm": 0.664918839931488,
      "learning_rate": 5.475215719129983e-07,
      "loss": 0.0974,
      "step": 940
    },
    {
      "epoch": 2.8369999999999997,
      "grad_norm": 0.37983039021492004,
      "learning_rate": 4.603396802224269e-07,
      "loss": 0.0718,
      "step": 945
    },
    {
      "epoch": 2.852,
      "grad_norm": 1.818696141242981,
      "learning_rate": 3.80649445240977e-07,
      "loss": 0.2286,
      "step": 950
    },
    {
      "epoch": 2.867,
      "grad_norm": 0.6052716970443726,
      "learning_rate": 3.0847519541800653e-07,
      "loss": 0.1475,
      "step": 955
    },
    {
      "epoch": 2.882,
      "grad_norm": 0.3851836323738098,
      "learning_rate": 2.438389646649858e-07,
      "loss": 0.0941,
      "step": 960
    },
    {
      "epoch": 2.8970000000000002,
      "grad_norm": 0.10997544974088669,
      "learning_rate": 1.8676048562880078e-07,
      "loss": 0.2691,
      "step": 965
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.9312155842781067,
      "learning_rate": 1.372571836676162e-07,
      "loss": 0.1715,
      "step": 970
    },
    {
      "epoch": 2.927,
      "grad_norm": 0.6943345665931702,
      "learning_rate": 9.534417153114206e-08,
      "loss": 0.0968,
      "step": 975
    },
    {
      "epoch": 2.942,
      "grad_norm": 0.3021044135093689,
      "learning_rate": 6.103424474688258e-08,
      "loss": 0.1716,
      "step": 980
    },
    {
      "epoch": 2.957,
      "grad_norm": 0.586799144744873,
      "learning_rate": 3.433787771382202e-08,
      "loss": 0.1614,
      "step": 985
    },
    {
      "epoch": 2.972,
      "grad_norm": 1.1655399799346924,
      "learning_rate": 1.5263220504724285e-08,
      "loss": 0.1641,
      "step": 990
    },
    {
      "epoch": 2.987,
      "grad_norm": 0.26816168427467346,
      "learning_rate": 3.816096378012057e-09,
      "loss": 0.0363,
      "step": 995
    },
    {
      "epoch": 2.999,
      "step": 999,
      "total_flos": 1.456365683394478e+17,
      "train_loss": 0.00920100252072255,
      "train_runtime": 452.1644,
      "train_samples_per_second": 6.635,
      "train_steps_per_second": 2.209
    }
  ],
  "logging_steps": 5,
  "max_steps": 999,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 20,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.456365683394478e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
